<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>WebModelDelivery — LLM Harness</title>
  <!-- ═══════════════════════════════════════════════════════════════════════════
       CONFIGURATION — All external URLs in one place.
       To switch between CDN and local hosting, swap the URLs below.
       ═══════════════════════════════════════════════════════════════════════════ -->
  <script>
    // ── Library dependencies ──────────────────────────────────────────────────
    // To host locally: download the files, serve from localhost, swap URLs here.
    //   e.g.  wllamaJs: './wllama/esm/index.js'
    //   e.g.  wllamaWasmST: './wllama/esm/single-thread/wllama.wasm'
    //   e.g.  wllamaWasmMT: './wllama/esm/multi-thread/wllama.wasm'
    const DEPS = {
      wllamaJs:      'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.3.7/esm/index.js',
      wllamaWasmST:  'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.3.7/src/single-thread/wllama.wasm',
      wllamaWasmMT:  'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.3.7/src/multi-thread/wllama.wasm',
    };

    // ── Model flat-repo sources ───────────────────────────────────────────────
    // Each URL points to a directory containing filemap.json + flat shard files.
    // The Service Worker intercepts /models/llm/* requests from wllama,
    // looks up the virtual path in filemap.json, fetches the real shard(s) from
    // cdnBase, reassembles them, and returns a same-origin Response (COEP-safe).
    //
    // To use locally hosted models instead of CDN, point to your local server:
    //   llm: `${location.origin}/pkg-gemma3`,
    //
    // ── Multimodal / mmproj (vision adapter) ──────────────────────────────────
    // If your model uses a vision adapter (mmproj), package it as a separate
    // flat-repo and add it here. wllama 2.x does not yet have native mmproj
    // support, but the SW can still deliver the file. You would:
    //   1. Package the mmproj GGUF with model-packager.sh into its own flat-repo
    //   2. Add a source here:
    //        mmproj: 'https://cdn.jsdelivr.net/gh/you/wmd_your-mmproj@v1/...',
    //   3. Add a pathPrefix in sendInit():
    //        { pathPrefix: '/models/mmproj/', cdnBase: MODEL_SOURCES.mmproj, ... }
    //   4. Load it via: fetch('/models/mmproj/your-mmproj.gguf') → SW delivers
    //   5. Pass the fetched blob to your vision pipeline alongside the LLM
    //
    const MODEL_SOURCES = {
      llm: 'https://cdn.jsdelivr.net/gh/kulogix/wmd_unsloth_gemma-3n-E2B-it-GGUF@v1.0.0',
      // mmproj: 'https://cdn.jsdelivr.net/gh/you/wmd_your-model-mmproj@v1.0.0',
    };
  </script>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: { extend: { fontFamily: { mono: ['"JetBrains Mono"', 'Consolas', 'monospace'] } } }
    };
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&display=swap" rel="stylesheet">
  <style>
    [x-cloak] { display: none !important; }
    .log-scroll { scrollbar-width: thin; scrollbar-color: #475569 transparent; }
    .log-scroll::-webkit-scrollbar { width: 6px; }
    .log-scroll::-webkit-scrollbar-thumb { background: #475569; border-radius: 3px; }
    @keyframes pulse-bar { 0%, 100% { opacity: 1; } 50% { opacity: 0.6; } }
    .bar-pulse { animation: pulse-bar 1.5s ease-in-out infinite; }
    .chat-bubble { word-break: break-word; white-space: pre-wrap; }
  </style>
</head>
<body class="bg-slate-950 text-slate-200 font-mono min-h-screen">

<div class="max-w-4xl mx-auto p-4 sm:p-6" x-data="llmHarness()" x-init="init()" x-cloak>

  <!-- Header -->
  <div class="mb-6">
    <h1 class="text-xl font-semibold text-white flex items-center gap-2">
      <span class="text-amber-400">&#9670;</span> WebModelDelivery — LLM Harness
    </h1>
    <p class="text-xs text-slate-500 mt-1">
      GGUF chat inference via wllama + Service Worker model delivery
      &middot; <span x-text="swStatus" class="text-slate-400"></span>
    </p>
  </div>

  <!-- ─── MODEL CARD ─────────────────────────────────────────────── -->
  <div class="bg-slate-900 border border-slate-800 rounded-lg p-4 mb-4">
    <div class="flex items-center justify-between flex-wrap gap-2 mb-3">
      <div class="flex items-center gap-2">
        <h2 class="text-sm font-semibold text-white">LLM Model</h2>
        <span x-show="modelReady" class="text-[10px] bg-emerald-900/60 text-emerald-400 px-2 py-0.5 rounded">LOADED</span>
        <span x-show="ggufName" class="text-[10px] text-slate-600" x-text="ggufName"></span>
      </div>
      <div class="flex items-center gap-2">
        <select x-model="manifest" @change="sendInit()" class="text-xs bg-slate-800 border border-slate-700 rounded px-2 py-1 text-slate-300" :disabled="modelLoading">
          <template x-for="opt in manifestOpts" :key="opt.value">
            <option :value="opt.value" x-text="opt.label"></option>
          </template>
        </select>
        <button @click="loadModel()" :disabled="modelLoading" class="text-xs bg-blue-600 hover:bg-blue-500 disabled:opacity-40 text-white px-3 py-1 rounded">
          <span x-show="!modelLoading">Load</span><span x-show="modelLoading">Loading&hellip;</span>
        </button>
        <button x-show="modelReady" @click="unloadModel()" class="text-xs bg-slate-700 hover:bg-slate-600 text-white px-3 py-1 rounded">Unload</button>
      </div>
    </div>

    <!-- Progress -->
    <div x-show="modelLoading || modelReady" class="grid grid-cols-2 gap-3 mb-3">
      <div class="bg-slate-950 rounded p-2">
        <div class="text-[10px] text-slate-500 uppercase tracking-wide mb-1">Service Worker</div>
        <div class="h-1.5 bg-slate-800 rounded overflow-hidden">
          <div class="h-full bg-blue-500 rounded transition-all duration-300" :class="modelLoading && !sw.done ? 'bar-pulse' : ''"
               :style="`width:${sw.percent}%`"></div>
        </div>
        <div class="text-[10px] text-slate-500 mt-1">
          <span x-text="sw.percent+'%'"></span> &middot; <span x-text="fmtB(sw.loaded)"></span>/<span x-text="fmtB(sw.total)"></span>
        </div>
      </div>
      <div class="bg-slate-950 rounded p-2">
        <div class="text-[10px] text-slate-500 uppercase tracking-wide mb-1">wllama</div>
        <div class="h-1.5 bg-slate-800 rounded overflow-hidden">
          <div class="h-full bg-amber-500 rounded transition-all duration-300" :style="`width:${wlPct}%`"></div>
        </div>
        <div class="text-[10px] text-slate-500 mt-1"><span x-text="wlStatus"></span></div>
      </div>
    </div>

    <div x-show="modelReady" class="text-[10px] text-slate-600" x-text="modelInfo"></div>
  </div>

  <!-- ─── CHAT ───────────────────────────────────────────────────── -->
  <div x-show="modelReady" class="bg-slate-900 border border-slate-800 rounded-lg p-4 mb-4">
    <div class="flex items-center justify-between mb-3">
      <h2 class="text-sm font-semibold text-white">Chat</h2>
      <div class="flex items-center gap-2">
        <label class="text-[10px] text-slate-500">Max tokens</label>
        <input type="number" x-model.number="maxTokens" class="text-xs bg-slate-800 border border-slate-700 rounded px-2 py-0.5 w-16 text-slate-300" min="1" max="4096" />
        <button @click="messages=[]; chatOutput=''" class="text-[10px] bg-slate-800 hover:bg-slate-700 text-slate-400 px-2 py-0.5 rounded">Clear Chat</button>
      </div>
    </div>

    <!-- Messages -->
    <div class="bg-slate-950 rounded p-3 max-h-80 overflow-y-auto log-scroll mb-3 space-y-2" x-ref="chatBox">
      <template x-for="(msg, i) in messages" :key="i">
        <div class="flex gap-2" :class="msg.role === 'user' ? 'justify-end' : ''">
          <div class="chat-bubble text-xs px-3 py-2 rounded-lg max-w-[80%]"
               :class="msg.role === 'user'
                 ? 'bg-blue-900/40 text-blue-200 rounded-br-sm'
                 : 'bg-slate-800 text-slate-300 rounded-bl-sm'"
               x-text="msg.content"></div>
        </div>
      </template>
      <div x-show="generating" class="flex gap-2">
        <div class="chat-bubble text-xs px-3 py-2 rounded-lg rounded-bl-sm bg-slate-800 text-slate-300 max-w-[80%]">
          <span x-text="streamText || '…'"></span>
          <span class="inline-block w-1 h-3 bg-amber-400 ml-0.5 animate-pulse"></span>
        </div>
      </div>
      <div x-show="!messages.length && !generating" class="text-slate-700 text-xs italic">Send a message to start chatting...</div>
    </div>

    <!-- Input -->
    <div class="flex gap-2">
      <input type="text" x-model="userInput" @keydown.enter="send()"
             :disabled="generating"
             class="flex-1 text-xs bg-slate-800 border border-slate-700 rounded px-3 py-2 text-slate-300 placeholder-slate-600"
             placeholder="Type a message..." />
      <button @click="send()" :disabled="generating || !userInput.trim()"
              class="text-xs bg-amber-700 hover:bg-amber-600 disabled:opacity-40 text-white px-4 py-2 rounded">
        <span x-show="!generating">Send</span><span x-show="generating">Generating&hellip;</span>
      </button>
      <button x-show="generating" @click="stopGen()" class="text-xs bg-red-700 hover:bg-red-600 text-white px-3 py-2 rounded">Stop</button>
    </div>
    <div x-show="genStats" class="text-[10px] text-slate-600 mt-1" x-text="genStats"></div>
  </div>

  <!-- ─── LOG ────────────────────────────────────────────────────── -->
  <div class="bg-slate-900 border border-slate-800 rounded-lg p-4">
    <div class="flex items-center justify-between mb-2">
      <h2 class="text-xs font-semibold text-slate-400 uppercase tracking-wide">Log</h2>
      <div class="flex gap-2">
        <button @click="clearCache()" class="text-[10px] bg-amber-800/60 hover:bg-amber-700/60 text-amber-300 px-2 py-0.5 rounded">Clear SW Cache</button>
        <button @click="log=[]" class="text-[10px] bg-slate-800 hover:bg-slate-700 text-slate-400 px-2 py-0.5 rounded">Clear Log</button>
      </div>
    </div>
    <div class="bg-slate-950 rounded p-2 max-h-48 overflow-y-auto log-scroll text-[11px] leading-relaxed space-y-0.5" x-ref="logBox">
      <template x-for="(entry, i) in log" :key="i">
        <div :class="{ 'text-slate-500': entry.t==='sys', 'text-blue-400': entry.t==='sw', 'text-amber-400': entry.t==='wl', 'text-red-400': entry.t==='err' }">
          <span class="text-slate-700" x-text="entry.ts"></span> <span x-text="entry.m"></span>
        </div>
      </template>
      <div x-show="!log.length" class="text-slate-700 italic">Waiting for events...</div>
    </div>
  </div>

  <p class="text-[10px] text-slate-700 mt-3 text-center">
    Multi-thread requires COOP/COEP headers &mdash; use <code class="text-slate-600">python3 cors_server.py</code> or <code class="text-slate-600">node serve.mjs</code>
    &middot; model-sw.js proxies cross-origin responses for COEP compatibility
  </p>
</div>

<script>
// ── Load wllama (URL from DEPS config above) ─────────────────────────────────
//
// NOTE: This must NOT be type="module" — modules always defer and would run
// AFTER Alpine's defer script, causing "Can't find variable" errors.
//
// HOW MODEL LOADING WORKS:
//
// 1. wllama calls loadModelFromUrl('/models/llm/my-model.gguf')
// 2. The Service Worker intercepts that request path
// 3. SW looks up 'my-model.gguf' in filemap.json (fetched from MODEL_SOURCES.llm)
// 4. filemap says it has 51 shards → SW fetches each shard from the CDN (or localhost)
// 5. SW streams the reassembled bytes back as a single Response
// 6. wllama receives a seamless GGUF file, never knows about shards or CDN
//
// The same mechanism works for any file the model needs (tokenizer, config, etc.)
//
// ── mmproj (multimodal vision adapter) ────────────────────────────────────────
//
// To load an mmproj alongside the main GGUF:
//
//   // 1. Add mmproj source in MODEL_SOURCES and sendInit() (see config block above)
//   // 2. After loadModel(), fetch the mmproj through the SW:
//   //
//   //   const mmprojResp = await fetch('/models/mmproj/your-mmproj-f16.gguf');
//   //   const mmprojBlob = await mmprojResp.blob();
//   //
//   // 3. When wllama adds mmproj support, pass it during loadModel:
//   //
//   //   await this._wl.loadModelFromUrl(modelUrl, {
//   //     ...config,
//   //     mmproj: '/models/mmproj/your-mmproj-f16.gguf',
//   //   });
//   //
//   // 4. For current wllama, you can manually handle the blob for vision tasks.
//   //    The key point is the SW transparently delivers the mmproj file from
//   //    CDN shards just like it does for the main GGUF.
//
let Wllama;
const wlReady = (async () => {
  // Fetch + patch wllama to fix signed-pointer overflow when WASM heap > 2GB.
  // wllama's embedded worker code treats WASM pointers as signed i32.  When the
  // heap grows past 2^31 bytes (common with large GGUF models), mmapAlloc and
  // wllamaMalloc return pointers that JavaScript interprets as negative,
  // crashing Uint8Array construction.  Fix: coerce pointers to unsigned via >>> 0.
  const resp = await fetch(DEPS.wllamaJs);
  let src = await resp.text();
  src = src.replace(
    'const ptr = m.mmapAlloc(size);',
    'const ptr = (m.mmapAlloc(size)) >>> 0;'
  );
  src = src.replace(
    'const inputPtr = await wllamaMalloc(argEncodedMsg.byteLength, 0);',
    'const inputPtr = (await wllamaMalloc(argEncodedMsg.byteLength, 0)) >>> 0;'
  );
  src = src.replace(
    'const outputPtr = await wllamaAction(argAction, inputPtr);',
    'const outputPtr = (await wllamaAction(argAction, inputPtr)) >>> 0;'
  );
  const blob = new Blob([src], { type: 'text/javascript' });
  const url = URL.createObjectURL(blob);
  const mod = await import(url);
  URL.revokeObjectURL(url);
  Wllama = mod.Wllama;
  return mod;
})();

// ── URL-derived path prefix ──────────────────────────────────────────────────
// Stable prefix from URL → TF.js/SW cache hits on reload, no collisions.
function urlToPrefix(url) {
  const parts = url.replace(/\/+$/, '').split('/').filter(Boolean);
  let slug = parts[parts.length - 1] || 'model';
  slug = slug.replace(/@/g, '_').replace(/[^a-zA-Z0-9._-]/g, '_');
  let h = 0;
  for (let i = 0; i < url.length; i++) { h = ((h << 5) - h + url.charCodeAt(i)) | 0; }
  const hash = (h >>> 0).toString(36).slice(0, 4).padStart(4, '0');
  return `/models/${slug}_${hash}/`;
}

const MODEL_PREFIXES = {
  llm: urlToPrefix(MODEL_SOURCES.llm),
};

// ── Wllama helpers (fully outside Alpine to avoid Proxy contamination) ───────
// Alpine deeply wraps x-data objects in reactive Proxies.  Even local variables
// obtained inside an Alpine async method can end up Proxy-wrapped when the
// async continuation resumes inside Alpine's effect scope.  When wllama's
// internal ProxyToWorker calls postMessage, structured-clone chokes on Proxy
// objects → DataCloneError.
//
// FIX: ALL wllama API calls live in plain async functions defined here, outside
// any Alpine component.  Alpine methods call these and pass plain callbacks for
// UI updates.  The wllama instance never appears inside Alpine's scope at all.

let _wl = null;   // raw Wllama instance — never touched by Alpine
let _fm = null;    // filemap object

async function _wlLoad(modelUrl, onProgress) {
  await wlReady;
  if (_wl) { try { await _wl.exit(); } catch(_) {} _wl = null; }
  const pathConfig = Object.create(null);
  pathConfig['single-thread/wllama.wasm'] = DEPS.wllamaWasmST;
  pathConfig['multi-thread/wllama.wasm'] = DEPS.wllamaWasmMT;
  _wl = new Wllama(pathConfig);

  const opts = Object.create(null);
  opts.n_ctx = 2048;
  opts.n_threads = navigator.hardwareConcurrency ? Math.min(navigator.hardwareConcurrency, 4) : 2;
  opts.useCache = false;
  if (onProgress) opts.progressCallback = onProgress;

  await _wl.loadModelFromUrl(modelUrl, opts);
}

async function _wlChat(messagesPlain, maxTokens, onToken) {
  if (!_wl) throw new Error('Model not loaded');
  const msgs = JSON.parse(JSON.stringify(messagesPlain));  // deep-plain copy
  const opts = Object.create(null);
  opts.nPredict = maxTokens;
  const sampling = Object.create(null);
  sampling.temp = 0.7; sampling.top_p = 0.9; sampling.top_k = 40;
  opts.sampling = sampling;
  if (onToken) opts.onNewToken = onToken;
  return await _wl.createChatCompletion(msgs, opts);
}

async function _wlUnload() {
  if (_wl) { try { await _wl.exit(); } catch(_) {} _wl = null; }
}

function _wlAbort() {
  if (_wl && _wl.abort) _wl.abort();
}

// ── Alpine component ─────────────────────────────────────────────────────────
window.llmHarness = () => ({
  swStatus: 'initializing…', log: [],
  manifest: '', manifestOpts: [{value:'',label:'Loading…'}],
  modelLoading: false, modelReady: false,
  sw: {percent:0,loaded:0,total:0,done:false},
  wlPct: 0, wlStatus: 'idle',
  ggufName: '', modelInfo: '',
  // Chat
  messages: [], userInput: '', streamText: '',
  generating: false, genStats: '', maxTokens: 512,

  async init() {
    if (this._initDone) return;   // guard against Alpine re-firing
    this._initDone = true;
    this.L('sys', 'Loading wllama...');
    try { await wlReady; } catch(e) { this.L('err', 'wllama load failed: '+e.message); return; }
    this.L('sys', 'wllama ready. Registering Service Worker...');

    if (!('serviceWorker' in navigator)) { this.L('err', 'Service Workers not supported'); return; }
    await navigator.serviceWorker.register('/model-sw.js', { scope: '/' });
    await navigator.serviceWorker.ready;
    if (!navigator.serviceWorker.controller) {
      this.L('sys', 'SW installed — reloading to activate...');
      location.reload(); return;
    }

    navigator.serviceWorker.addEventListener('message', (e) => {
      const d = e.data;
      if (d?.type === 'MODEL_SW_PROGRESS' && d.pathPrefix === MODEL_PREFIXES.llm) {
        this.sw = { percent: d.percent, loaded: d.modelLoaded, total: d.modelTotal, done: d.done };
      }
      if (d?.type === 'MODEL_SW_CACHE_CLEARED') this.L('sys', 'SW cache cleared.');
    });

    await this.fetchFM();
    this.sendInit();
    this.swStatus = 'SW active';
    this.L('sys', 'Ready. Model source: ' + (MODEL_SOURCES.llm.includes('localhost') || MODEL_SOURCES.llm.includes('127.0.0.1') ? 'local' : 'CDN'));
  },

  async fetchFM() {
    try {
      const r = await fetch(MODEL_SOURCES.llm + '/filemap.json');
      if (!r.ok) throw new Error(`HTTP ${r.status}`);
      const fm = await r.json();
      _fm = fm;
      const names = Object.keys(fm.manifests || {});
      this.manifestOpts = names.map(n => ({ value: n, label: `${n} (${this.fmtB(fm.manifests[n].size)})` }));
      if (!this.manifestOpts.length) this.manifestOpts = [{value:'',label:'none'}];
      this.manifest = names[0] || '';
      this.ggufName = Object.keys(fm.files).find(f => f.endsWith('.gguf')) || '';
      this.L('sys', `LLM filemap: ${names.join(', ')} — GGUF: ${this.ggufName || 'none'}`);
    } catch(e) {
      this.manifestOpts = [{value:'',label:'Error'}];
      this.L('err', 'LLM filemap: '+e.message);
    }
  },

  // ── SW config ────────────────────────────────────────────
  // cdnBase = MODEL_SOURCES URL (CDN or localhost).
  sendInit() {
    const sources = [
      { pathPrefix: MODEL_PREFIXES.llm, cdnBase: MODEL_SOURCES.llm, progress: true, manifest: this.manifest },
    ];
    // If you have an mmproj source, add it here:
    // if (MODEL_SOURCES.mmproj) {
    //   sources.push({ pathPrefix: '/models/mmproj/', cdnBase: MODEL_SOURCES.mmproj, progress: false });
    // }
    navigator.serviceWorker.controller?.postMessage({ type: 'MODEL_SW_INIT', sources });
  },

  // ── Load model ───────────────────────────────────────────
  async loadModel() {
    if (!this.ggufName) { this.L('err', 'No GGUF file found in filemap'); return; }
    if (this.modelReady) await this.unloadModel();
    this.modelLoading = true;
    this.wlPct = 0; this.wlStatus = 'initializing…';
    this.sw = {percent:0,loaded:0,total:0,done:false};

    const t0 = performance.now();
    const modelUrl = `${location.origin}${MODEL_PREFIXES.llm}${this.ggufName}`;
    this.L('sys', `Loading ${this.ggufName} via SW...`);

    // Capture Alpine refs as plain vars for the callback
    const self = this;
    const progressCb = function({ loaded, total }) {
      const pct = total > 0 ? Math.round(loaded / total * 100) : 0;
      self.wlPct = pct;
      self.wlStatus = `${pct}% (${self.fmtB(loaded)}/${self.fmtB(total)})`;
    };

    try {
      // _wlLoad runs entirely outside Alpine's scope
      await _wlLoad(modelUrl, progressCb);

      const ms = (performance.now() - t0) | 0;
      this.modelReady = true;
      this.wlPct = 100;
      this.wlStatus = `ready ✓ ${ms}ms`;
      this.modelInfo = `Loaded in ${(ms/1000).toFixed(1)}s`;
      navigator.serviceWorker.controller?.postMessage({ type: 'MODEL_SW_COMPLETE', pathPrefix: MODEL_PREFIXES.llm });
      this.L('sys', `Model loaded in ${(ms/1000).toFixed(1)}s`);
    } catch(e) {
      this.wlStatus = 'error: ' + e.message;
      this.L('err', 'Model load: ' + e.message);
      console.error(e);
    }
    this.modelLoading = false;
  },

  async unloadModel() {
    await _wlUnload();
    this.modelReady = false; this.messages = []; this.streamText = '';
    this.L('sys', 'Model unloaded.');
  },

  // ── Chat ─────────────────────────────────────────────────
  async send() {
    const text = this.userInput.trim();
    if (!text || !_wl || this.generating) return;
    this.userInput = '';
    this.messages.push({ role: 'user', content: text });
    this.generating = true;
    this.streamText = '';
    this.genStats = '';
    this.$nextTick(() => { const el = this.$refs.chatBox; if(el) el.scrollTop = el.scrollHeight; });

    const t0 = performance.now();
    let tokenCount = 0;
    try {
      // Build plain messages array for _wlChat (outside Alpine scope)
      const plainMsgs = this.messages.map(m => ({ role: m.role === 'user' ? 'user' : 'assistant', content: m.content }));
      const maxTok = this.maxTokens;
      const self = this;
      const tokenCb = function(token, piece, currentText) {
        tokenCount++;
        self.streamText = currentText;
        const el = self.$refs.chatBox; if (el) el.scrollTop = el.scrollHeight;
      };

      const result = await _wlChat(plainMsgs, maxTok, tokenCb);

      const ms = performance.now() - t0;
      const reply = result || this.streamText;
      this.messages.push({ role: 'assistant', content: reply });
      this.streamText = '';
      const tps = tokenCount > 0 ? (tokenCount / (ms / 1000)).toFixed(1) : '?';
      this.genStats = `${tokenCount} tokens · ${(ms/1000).toFixed(1)}s · ${tps} t/s`;
      this.L('sys', `Generated ${tokenCount} tokens at ${tps} t/s`);
    } catch(e) {
      if (e.name !== 'AbortError') {
        this.L('err', 'Generation: ' + e.message);
        if (this.streamText) { this.messages.push({ role: 'assistant', content: this.streamText + ' [error]' }); this.streamText = ''; }
      }
    }
    this.generating = false;
    this.$nextTick(() => { const el = this.$refs.chatBox; if(el) el.scrollTop = el.scrollHeight; });
  },

  stopGen() {
    _wlAbort();
    this.L('sys', 'Generation stopped by user.');
  },

  // ── Helpers ──────────────────────────────────────────────
  fmtB(b) { return !b?'0 B':b>=1048576?(b/1048576).toFixed(1)+' MB':b>=1024?(b/1024).toFixed(1)+' KB':b+' B'; },
  clearCache() { navigator.serviceWorker.controller?.postMessage({type:'MODEL_SW_CLEAR_CACHE'}); this.L('sys','Clearing cache...'); },
  L(t,m) {
    this.log.push({t,m,ts:new Date().toTimeString().slice(0,8)});
    if(this.log.length>300) this.log.splice(0,50);
    this.$nextTick(()=>{ const el=this.$refs.logBox; if(el) el.scrollTop=el.scrollHeight; });
  },
});
</script>
<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js"></script>
</body>
</html>
